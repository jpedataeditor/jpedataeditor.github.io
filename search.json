[
  {
    "objectID": "de.html",
    "href": "de.html",
    "title": "The JPE Data Editor",
    "section": "",
    "text": "Starting with July 1st 2025, the Data Editor of the JPE is Florian Oswald.",
    "crumbs": [
      "Data Editor"
    ]
  },
  {
    "objectID": "policy.html",
    "href": "policy.html",
    "title": "JPE Data and Code Policy",
    "section": "",
    "text": "It is the policy of the Journal of Political Economy (JPE) to publish papers only if the data and computer code used in the analysis are clearly and precisely documented and are readily available to any researcher for purposes of replication (unless the exemptions discussed below apply). Authors of conditionally accepted papers, in particular those that contain empirical work, simulations, experimental work, or numerical computations, must provide, prior to publication, the data, programs, and other details of the computations sufficient to permit replication. They must also provide sufficient information to replicate the process of obtaining the raw data from the original sources and cite all the sources of data appropriately.\nThe JPE will perform reproducibility checks of empirical, experimental, and simulation results of all conditionally accepted papers and their approved online appendices. Eventual publication of the paper is conditional on a positive outcome of those checks. Requests for an exemption from providing the materials described in this policy, or for restricting their usage, should be stated clearly when the paper is first submitted for review. The handling editor will decide whether the paper should be reviewed in this case. Exceptions will not be considered later in the review and publication process.\nBy submitting to the JPE, authors indicate their acceptance of this Data and Code Policy.\nThe JPE endorses DCAS, the Data and Code Availability Standard [v1.0], and its data and code availability policy is compatible with DCAS.\nThe specific terms of the JPE Data and Code Policy are as follows.",
    "crumbs": [
      "Code and Data Policy"
    ]
  },
  {
    "objectID": "policy.html#data",
    "href": "policy.html#data",
    "title": "JPE Data and Code Policy",
    "section": "1 Data",
    "text": "1 Data\n\n1.1 Data Availability Statement\nA Data Availability Statement (DAS) must be provided with sufficient detail for independent researchers to replicate the necessary steps to access the original data, including information on any limitations and the expected monetary and time costs associated with data access. When applicable, the DAS should also specify the version of the dataset and the original date of access by the authors. Similarly, the DAS should clearly indicate which datasets are included and excluded from the replication package. The DAS should be included as a section of the README file (see Section 3.4 below).\n\n\n1.2 Raw Data\nThe raw data utilized in the research, including primary data collected by the author and secondary data, must be included in the replication package. If the exact extract of the raw data used in the analysis is published in a trusted repository that satisfies the FAIR data principles (guidance here), including a permanent identifier (e.g., its DOI) linking to these raw data is considered sufficient to fulfill the obligation of including the raw data in the replication package. In cases where legal barriers for sharing the data prohibit the authors from publishing them, the authors must request an exemption and provide a justification for not complying with the policy. When exemptions are granted, authors are required to comply with at least one of the following two procedures:\n\nWhenever possible, provide the JPE with temporary access to the data affected by the exemption for the purpose of implementing reproducibility checks. It is the authors’ responsibility to obtain permission from the data provider to confidentially share the data with the JPE.\nInclude in the replication package a synthetic or simulated dataset that enables users to execute the code and verify that it generates all outputs presented in the paper and appendices, even if the results differ from those in the paper. While including synthetic/simulated data is not required when temporary access for JPE is provided, it is still recommended, as it allows future users of the package to run the codes, increasing the publication’s impact.\n\nIn either of these two procedures, the authors are expected to clarify the nature of the exemption in the DAS (see Section 1.1).\n\n\n1.3 Analysis Data\nAnalysis Data is provided as part of the replication package unless they can be fully reproduced from accessible data within a reasonable time frame. Exceptions are to be explained in the DAS.\n\n\n1.4 Format\nThe data files must be either in plain ASCII format, such as comma-separated value (.csv), or any other non-proprietary format so that they can be read by any researcher on any machine. Additionally, the authors may choose to submit data in a format that is read by specific programs, such as Matlab (.mat) files, Stata (.dta), or Excel (.xlsx) files, but a copy of these files in a non-proprietary format is required in every case.\n\n\n1.5 Metadata\nA description of the variables included in the data and their allowed values must be made publicly accessible. Such a description could take the form of labels in the dataset, comments in the code, easy-to-identify variable names, codebooks, and indications in the README file.\n\n\n1.6 Citations\nAll data used in the paper and the approved online appendices must be appropriately cited in both the paper/appendices and in a dedicated references section of the README file. As a general guideline, citations of data employed in the paper should be included in the paper’s references section, while citations exclusively pertaining to data used in the approved online appendices may be relegated to the appendix. However, in exceptional circumstances, such as when there is a large number of data sources to cite or when recommended by the handling co-editor, citations of data used in the paper may be included in a references section of the approved appendix only.",
    "crumbs": [
      "Code and Data Policy"
    ]
  },
  {
    "objectID": "policy.html#code",
    "href": "policy.html#code",
    "title": "JPE Data and Code Policy",
    "section": "2 Code",
    "text": "2 Code\n\n2.1 Data Transformations\nAll programs used to generate final and analysis data sets from raw data must be included, even if the raw datasets cannot be provided due to approved exemptions to comply with Section 1.2 above. Whenever transformations or simulations include randomly generated numbers, the code must ensure reproducibility with appropriate means (e.g., setting a seed for the random number generator).\n\n\n2.2 Analysis Code\nPrograms that produce any kind of computational results (e.g., estimation, simulation, model solution, visualization, etc.) must be included. These programs should produce all such computational exhibits in the paper and approved online appendices (i.e., tables, figures, in-text numbers) with minimal human intervention.\nIn cases where execution times of the programs are excessively long, authors are encouraged to provide simplified versions of their programs that allow partial reproduction of results in a reasonable time frame. Similarly, authors of such computationally intensive papers are encouraged to include intermediate results in the replication package, such that reproducibility checks can be performed, taking those results as given. Note that this does not invalidate the requirement to provide code that reproduces all output without relying on intermediate results. In such cases, authors are required to collaborate with the Data Editor to develop a feasible strategy for reproducibility checks. The extent of the eventual partial reproducibility checks must be clearly documented in the package README.\n\n\n2.3 Format of Code\nCodes must be provided in source format which can be directly interpreted or compiled by appropriate software. Single-driver scripts that run all of the code from raw data to final results are strongly encouraged and must be provided at the Data Editor’s specific request (e.g., to limit the number of human intervention steps). The code must save all exhibits (e.g., tables and figures) appearing in the paper and appendices in a specified directory within the replication package. When the codes are written in compiled languages, precise instructions for all steps and compiling options must be included in the documentation. A make file (or similar build tool) that reproduces compilation steps is strongly encouraged. In general, all necessary steps to recreate a stable computational environment should be taken (such as, e.g., precise specification of software and library versions, virtual environments, and containerization). Software that does not allow generating output using scripts (e.g., ArcGIS or MS Excel) is discouraged. When this type of software is used, very precise step-by-step instructions allowing users to exactly reproduce the generated outputs independently of the authors must be included in the README file.",
    "crumbs": [
      "Code and Data Policy"
    ]
  },
  {
    "objectID": "policy.html#supporting-materials",
    "href": "policy.html#supporting-materials",
    "title": "JPE Data and Code Policy",
    "section": "3 Supporting Materials",
    "text": "3 Supporting Materials\n\n3.1 Survey Instruments and Experimental Instructions\n\nMaterial Expected in Replication Package\nIn case the raw data are collected or generated via surveys or experiments, authors are required to include survey instruments or experimental instructions and subject selection criteria in the replication package. Specifically, it is required that the entirety of the following information be included in the package README, regardless of whether some of it appears in the paper or the appendix already:\n\nThe subject pool and recruiting procedures.\nThe experimental technology – when and where the experiments were conducted; by computer or manually; online, and so forth.\nAny procedures to test for comprehension before running the experiment, including the use of practice trials and quizzes.\nMatching procedures, especially for game theory experiments.\nSubject payments, including whether artificial currency was used, the exchange rate, show-up fees, average earnings, lotteries, and/or grades.\nThe number of subjects used in each session and, where relevant, their experience.\nTiming, such as how long a typical session lasted, and how much of that time was instructional.\nAny use of deception and/or any instructional inaccuracies.\nDetailed statement of protocols.\nSamples of permission forms and record sheets.\nCopies of instructions and slides/transparencies used to present instructions.\nSource code for computer programs used to conduct the experiment and to analyze the data. This does not include compilers (such as zTree) that are publicly available.\nScreen shots showing how the programs are used.\n\nThe documentation provided in the replication package must be self-contained, regardless of the content included in the paper and approved online appendices, as the replication package is a different citable object than the paper. As a general rule, the replication package must be at least as exhaustive as the paper and approved appendices, since there are no space constraints in the documentation provided in the replication package.\n\n\nInitial Submission of Experimental Papers\nSimilarly to requests about exemptions from the Data and Code Policy, authors should include as much information about their experimental procedure as possible in their initial submission. All the materials listed above are desirable and typically expected, but further details about what is needed in each case can be obtained from the co-editor handling the paper. If, during the review process, the editor or referees feel additional information is needed, requests for that material will be made and may naturally cause a delay in processing. Hence, we encourage as complete a submission as feasible.\n\n\n\n3.2 Ethics\nAuthors who collect primary data (e.g., via experiment or survey) are required to include the IRB approval documentation (or similar) from their institution.\n\n\n3.3 Pre-registration\nIf applicable, pre-registration of the research project must be documented in the README.\n\n\n3.4 Documentation in README file\nA README document in either pdf or markdown format has to be included in the replication package. The file README.pdf or README.md must be located at the root of the replication package, it must contain all information required to reproduce the results (there should not be multiple locations for documentation). As a minimal requirement, the README should contain at least the following items:\n\nA DAS as explained in Section 1.1\nA description of the content of the replication package.\nPrecise instructions on all the steps required to run the code to reproduce all exhibits included in the paper and appendices.\nPrecise instructions on where in the replication package the produced outputs will be saved, and how each of them maps to the exhibits included in the paper and appendices.\nPrecise specifications of software and hardware used by the authors when preparing the package, including expected running time and specific requirements needed to successfully reproduce the results (e.g., software versions, libraries to be installed, etc.). When the requirements and execution time are heterogeneous across significant portions of the package, specific requirements and running times for each of the different parts must be indicated.\nData citations, according to Section 1.6.\n\nIt is strongly recommended to use the README template (available here) of the Social Sciences Data Editors.",
    "crumbs": [
      "Code and Data Policy"
    ]
  },
  {
    "objectID": "policy.html#sharing",
    "href": "policy.html#sharing",
    "title": "JPE Data and Code Policy",
    "section": "4 Sharing",
    "text": "4 Sharing\n\n4.1 Location\nBy default, conditionally accepted papers should deposit their replication package in the JPE dataverse repository, where it will be reviewed by the Data Editor. After the successful conclusion of reproducibility checks, the package will be published in this dataverse space.\nIn cases where data cannot be published in an openly accessible trusted data repository like the JPE dataverse, authors who have requested an exemption to publish them at the time of first submission must commit to preserving the data and code for a period of no less than five years following the publication of the paper. They should also provide reasonable assistance to requests for clarification and replication.\n\n\n4.2 License\nThe replication package must be deposited with a license that specifies the terms of use of the code and data in the replication package. The license must, at the very least, allow for unrestricted access to all files included in the deposit and permit the usage of the package for replication purposes by researchers unconnected to the original parties.\n\n\n4.3 Omissions\nThe README must clearly indicate any omission of the required parts of the package as a result of a granted exemption. The README must also indicate the reasons for such omission, such as legal requirements, limitations, or other approved agreements. In cases where the extent or complexity of the reproducibility checks impedes the exact reproduction of all the results in the paper and approved appendices, such as when synthetic datasets are provided (see Section 1.2) or partial checks have been implemented (see Section 2.2), the README must clarify which results have not been checked for reproducibility and why.\n\nVersion v1, June 5, 2025",
    "crumbs": [
      "Code and Data Policy"
    ]
  },
  {
    "objectID": "posts/20240505-stataversions/index.html",
    "href": "posts/20240505-stataversions/index.html",
    "title": "STATA Versioning",
    "section": "",
    "text": "This post is the first in a series where we want to analyse the behaviour of STATA in terms of cross-version reproducibility of results. In the current post, we will see inconsistent results for built-in STATA commands across versions, in a future post I plan to illlustrate problematic interactions with user-contributed code in the form of so-called .ado files (I will refer to those as packages). We will draw on real-world examples in the form of replication packages published together with academic papers in the Economic Journal. We will perform computational experiments by running identical code and data with different versions of STATA.\nThe starting point of this exploration is a blog post by the president of StataCorp from 2019 (with a similar message on the STATA website):\nWe will encounter below one example where we found behaviour which is incompatible with this statement. The aim of this post is not to discredit STATA in any way, but to provide hopefully useful advice on how to avoid distributing unreproducible code. In general, there is nothing which would make STATA better or worse suited to achieve reproducibility, and in this sense I humbly disagree with the above statement. Each language presents its own set of challenges, and being aware of those goes a long way. We reserve a comparison of computing languages in terms of being helpful to writing reproducible code for another occasion."
  },
  {
    "objectID": "posts/20240505-stataversions/index.html#stata-version-x-command",
    "href": "posts/20240505-stataversions/index.html#stata-version-x-command",
    "title": "STATA Versioning",
    "section": "Stata version x Command",
    "text": "Stata version x Command\nSTATA has a command version x which, if called, will mimick the STATA interpreter version x. The entry for help version gives a list of all known changes in behaviour if one sets version x, relative to the current version. The fact that this works is on the one hand nothing but remarkable - the code base of STATA must have changed considerably over the last few decades, surely countless bugs must have been fixed, etc, therefore ensuring that this works must require tremendous effort; on the other hand, it is obvious that version x does not replace the currently running STATA binary with the one of previous STATA version x, so any interactions of that binary with the current computing environment (OS, support libraries etc) could lead to unexpected behaviour. Indeed, we will document such behaviour below.\n\n\n\n\n\n\nData Editors do not care about backward compatibility\n\n\n\nIn general, Data Editors would not require that your code needs to be reproducible with all versions of STATA; quite on the contrary, we want a very specific description of the required environment. Another aim of this post is thus to clarify some (unexpected) effects of the version command, and to counter the belief that setting version will guarantee cross version reproducibility."
  },
  {
    "objectID": "posts/20240505-stataversions/index.html#case-study-xporegress-in-burchardi-et-al-ej-2024",
    "href": "posts/20240505-stataversions/index.html#case-study-xporegress-in-burchardi-et-al-ej-2024",
    "title": "STATA Versioning",
    "section": "Case Study: xporegress in Burchardi et al (EJ 2024)",
    "text": "Case Study: xporegress in Burchardi et al (EJ 2024)\n\nWe found different results for xporegress across STATA versions 16 and v 18 while checking the Burchardi et al (EJ 2024) paper. xporegress is a built-in STATA command.\nNotice we don’t know whether there is anything wrong in xporegress per se, or whether some other aspect of STATA code changed in a way which leads to different results across both verions. We are also agnostic about whether the authors were writing code in any way incompatible with STATA version 18, because:\nTo us, this is irrelevant, as we want to be able to rely on the promise embedded in version 16 solving all issues. The authors set version 16, as well as all required random seeds, in the correct fashion.\n\n\n\n\n\n\n\n🏆 Data Editor Award for Excellent Collaboration\n\n\n\nThe Burchardi et al package features high quality of code and exceptional collaboration on behalf of the authors. All results replicate exactly on STATA version 16, as indicated in their package readme. They were the first to point out the issue with xporegress to me. Thanks to them! 🤝\n\n\n\nStep 1: Safe Environment via docker\n\nWe want to minimize the risk that anything in our findings is driven by my local STATA installation, and its interaction with my OS. A good solution to this is a containerized environment, for example via docker.\nWe want to have 2 docker containers with version 16 and 18 respectively. We basically want to test the effect of the version 16 command on the stata 18 binary.\nThe dedicated repository of docker images provided by the AEA Data Editor Lars Vilhuber is going to be very helpful here. The prebuilt docker images for different stata versions are available here on dockerhub.\n\nWe can run STATA 16 and STATA 18 as if we had two separate machines inside an isolated environment (a container) on our own computer.\nNotice that there are no user contributed packages installed into those containers. Furthermore, Burchardi et al provide .ado files for add-on packages with their code. So, we have a clean slate.\n\n\n\n\nStep 2: Code to Run Experiment\nTo minimize further mistakes from manually handling code and results, I wrote some julia code which is available here. It downloads the Burchardi et al package from zenodo.org, unpacks it, modifies it slightly (so that it only produces the required appendix table B8 where the discrepancy arises), and then launches the respective STATA containers with the identical code and data; first version 16, where results correspond exactly to the paper, and then version 18, where they do not. The point being, given the promise contained in the version 16 command, results should correspond across versions.\nIf you follow instructions contained therein, you will see the following output in your terminal:\nfloswald@PTL11077 ~/g/E/b/stataversion (main)&gt; julia --project=. runblog.jl\n\n[ Info: downloading record Burchardi\n[ Info: content of /Users/floswald/git/EJData/blogs/stataversion/Burchardi/3-replication-package\n[\".DS_Store\", \"Code\", \"Data\", \"Materials\", \"Output\", \"README.pdf\"]\n[ Info: done downloading\n[ Info: run with stata 16\n[ Info: STATA version 16, docker IMG tag 2023-06-13\nWARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n[ Info: renaming /Users/floswald/git/EJData/blogs/stataversion/Burchardi/3-replication-package/Output/Tables/DEMED_AppendixTableB8_dummy.tex\n[ Info: renaming /Users/floswald/git/EJData/blogs/stataversion/Burchardi/3-replication-package/Output/Tables/DEMED_AppendixTableB8_index.tex\n[\"/Users/floswald/git/EJData/blogs/stataversion/Burchardi/3-replication-package/Output/Tables/DEMED_AppendixTableB8_dummy-v16.tex\", \"/Users/floswald/git/EJData/blogs/stataversion/Burchardi/3-replication-package/Output/Tables/DEMED_AppendixTableB8_index-v16.tex\"]\n[ Info: run with stata 18\n[ Info: STATA version 18, docker IMG tag 2024-04-30\nWARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n[ Info: done 👋\n\n\nStep 3: Results\nThe code repository also contains a short latex script which will compile the tables outputted from each docker container into a simple table. We can compare the result for each table across both versions. We can see for 2 columns a noticeable difference, both in estimates as well as in standard errors. The numbers in equally colored shapes should be identical.\n\n\n\nResults for Table B8 Panel A\n\n\n\n\n\n\nResults for Table B8 Panel B"
  },
  {
    "objectID": "posts/20240505-stataversions/index.html#conclusion",
    "href": "posts/20240505-stataversions/index.html#conclusion",
    "title": "STATA Versioning",
    "section": "Conclusion",
    "text": "Conclusion\nThe authors of Borrowing Constraints and Demand for Remedial Education: Evidence from Tanzania (forthcoming EJ 2024) have provided an excellent replication package which is available here. They alerted our team to a discrepancy arising from the xporegress command across STATA versions 16 vs 18. This is despite their correct usage of the version 16 command, and despite the insistence on the fact that everything will just work in the initial quote."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Journal of Political Economy  Data Editor Website",
    "section": "",
    "text": "This website provides all the relevant information regarding the Data and Code Availability Policy and pre-acceptance reproducibility checks implemented at the Journal of Political Economy.\nDraft of JPE Data and Code Policy - planned to go into effect on July 1st 2025.  \nFind out more about the JPE Data Editor here."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "STATA Versioning\n\n\n\nSTATA\n\n\nversions\n\n\npackages\n\n\n\nChallenges when reproducing results with different versions of STATA\n\n\n\nFlorian Oswald\n\n\nJun 5, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  }
]